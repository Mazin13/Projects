# etl_script.py
import pandas as pd
from sqlalchemy import create_engine

# Step 1: Extract - Load the California Housing dataset
from sklearn.datasets import fetch_california_housing
housing_data = fetch_california_housing(as_frame=True)
df = housing_data.frame

# Step 2: Transform - Clean and transform the data as needed
# For simplicity, we'll keep only a subset of columns
selected_columns = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal']
df = df[selected_columns]

# Step 3: Load - Load the data into a PostgreSQL database
# Replace with your PostgreSQL database information
connection_string = 'postgresql://your_user:your_password@localhost:5432/your_database'
engine = create_engine(connection_string)

# Write the DataFrame to a PostgreSQL table
table_name = 'housing_data'
df.to_sql(table_name, engine, index=False, if_exists='replace')

print(f'Data loaded into PostgreSQL table "{table_name}" successfully.')
